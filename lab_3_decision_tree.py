# -*- coding: utf-8 -*-
"""Lab 3 Decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gIcuwYZhuGwHxqlBOq6Bb1381GoJVFG3

# ***Predicting Maternal Health Risk Level***

---

# ***Table of Content***

1. [Introduction](#Introduction)
2. [Data Loading](#DL)
3. [Exploratory Data Analysis](#EDA)
4. [ Data Preprocessing](#DP)
5. [ Modelling](#M)
6. [ Model Evaluation](#ME)
7. [ Conclusion](#C)

<a name="Introduction"></a>
# ***1. Introduction***

The following notebook focuses on creating a Decision Tree Model that predicts the maternal health risk level. This is of great importance as it tells us if a particular individual will face pregnancy complications later on. The dataset used may be found on the UC Irvine Machine Learning Repository website (https://archive.ics.uci.edu/dataset/863/maternal+health+risk). It consists of 1013 rows and 7 columns which are as follows:

 - Age
 - Systolic blood pressure (SystolicBP) in mmHg
 - Diastolic blood pressure (DiastolicBP) in mmHg
 - Blood Sugar Levels (BS) in mmol/L  
 - Body Temperature (BodyTemp) in F
 - Heart Rate in bpm
 - Risk Level

<a name="DL"></a>
# ***2. Data Loading***
"""

import numpy as np
import pandas as pd
import os
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns

# def dataset_dir():
#     while True:
#         dataset_dir = input("Enter dataset directory: ")

#         if os.path.exists(dataset_dir):
#             break
#         else:
#             print("Oops!  No such directory :(")
#     return dataset_dir

# df = pd.read_csv(dataset_dir())

df = pd.read_csv("/content/Maternal Health Risk Data Set.csv")

"""<a name="EDA"></a>
# ***3. Exploratory Data Analysis***

## ***3.1 Data Overview***
"""

df.head() # Display first 5 rows

df.tail() # Display last 5 rows

df.info() # Display the basic information about the dataset

df.describe() # Display the basic stats about the dataset

"""## ***3.2 Visualizations***"""



"""<a name="DP"></a>
# ***4. Data Preprocessing***
"""

# Checking the number of missing values under each feature
print("Total null values at each column: \n", df.isnull().sum())

# Encoding the categorical target values
label = LabelEncoder()

# Adding a new column of the encoded target values
df['Target_column_label'] = label.fit_transform(df['RiskLevel'])

# Revisting the modified dataframe's first five rows
df.head()

"""<a name="M"></a>
# ***5. Modelling***

## ***5.1 Training the Model***
"""

# Storing the features in X and the target values in y
X = df.iloc[:, 0:6]
y = df.iloc[:, 7]

# Splitting the data into 70% training and 30% testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Assigning the Decision Tree Classfier instance to a variable
model = DecisionTreeClassifier(random_state=42)

# Training the model
model.fit(X_train, y_train)

# Testing the model by predicting the y values of the X test data
y_pred = model.predict(X_test)

"""## ***5.2 Hyperparameter Tuning***"""

# Creating a parameter grid to try and improve the Decision Tree model
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 4],
    'max_features': [None, 'sqrt', 'log2']
}

# First technique is using a grid search with 5 cross validation folds to tune the model
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Training the model to tune
grid_search.fit(X_train, y_train)

# Second technique is using a randomized search with 5 cross validation folds
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42)

# Training the model to tune
random_search.fit(X_train, y_train)

# Recover the tuned Decision Tree models with the best parameters
gridSearch_model = grid_search.best_estimator_
randomSearch_model = random_search.best_estimator_

# Testing the tuned Decision Tree models by predicting the y values of the X test data
y_pred_tuned1 = gridSearch_model.predict(X_test)
y_pred_tuned2 = randomSearch_model.predict(X_test)

"""## ***5.3. Model Evaluation***


"""

# Creating a model performance report for the Decision Tree Model
Model_performance = classification_report(y_test, y_pred)
print(f"The report for the Decision Tree Model: \n{Model_performance}\n")

# Creating a model performance report for the Decision Tree Model Tuned With Grid Search CV
Model_performance = classification_report(y_test, y_pred_tuned1)
print(f"The report for the Decision Tree Model Tuned With Grid Search CV: \n{Model_performance}\n")

# Creating a model performance report for the Decision Tree Model Tuned With Randomized Search CV
Model_performance = classification_report(y_test, y_pred_tuned2)
print(f"The report for the Decision Tree Model Tuned With Randomized Search CV: \n{Model_performance}")

# Creating a confusion matrix for the Decision Tree Model
cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot(cmap=plt.cm.plasma)
plt.title('Decision Tree')
disp.ax_.grid(False)
plt.show()

# Creating a confusion matrix for the Decision Tree Model Tuned with Grid Search CV
cm1 = confusion_matrix(y_test, y_pred_tuned1, labels=gridSearch_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=gridSearch_model.classes_)
disp.plot(cmap=plt.cm.inferno)
plt.title('Decision Tree Tuned with Grid Search CV')
disp.ax_.grid(False)
plt.show()

# Creating a confusion matrix for the Decision Tree Model Tuned with Randomized Search CV
cm2 = confusion_matrix(y_test, y_pred_tuned2, labels=randomSearch_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=randomSearch_model.classes_)
disp.plot(cmap=plt.cm.magma)
plt.title('Decision Tree Tuned with Randomized Search CV')
disp.ax_.grid(False)
plt.show()

# Creating a Presicion VS Recall curve for the Desicion Tree Model

y_pred = model.predict_proba(X_test)  # Predict probabilities for all classes
n_classes = len(model.classes_)

# Arrays to store precision, recall, and PR-AUC for each class
precisions = []
recalls = []
pr_aucs = []

# Calculate and plot PR curves for each class
plt.figure(figsize=(8, 6))

for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_test == i, y_pred[:, i])
    pr_auc = average_precision_score(y_test == i, y_pred[:, i])

    precisions.append(precision)
    recalls.append(recall)
    pr_aucs.append(pr_auc)

    plt.plot(recall, precision, lw=2, label='Class %d (AUC = %0.2f)' % (i, pr_auc))

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.title('Precision-Recall Curve for Desicion Tree')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

# Creating a Presicion VS Recall curve for the Desicion Tree Model Tuned with Grid Search CV

y_pred_tuned1 = gridSearch_model.predict_proba(X_test)  # Predict probabilities for all classes
n_classes = len(gridSearch_model.classes_)

# Arrays to store precision, recall, and PR-AUC for each class
precisions = []
recalls = []
pr_aucs = []

# Calculate and plot PR curves for each class
plt.figure(figsize=(8, 6))

for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_test == i, y_pred_tuned1[:, i])
    pr_auc = average_precision_score(y_test == i, y_pred_tuned1[:, i])

    precisions.append(precision)
    recalls.append(recall)
    pr_aucs.append(pr_auc)

    plt.plot(recall, precision, lw=2, label='Class %d (AUC = %0.2f)' % (i, pr_auc))

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.title('Precision-Recall Curve for Desicion Tree Tuned with Grid Search')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

# Creating a Presicion VS Recall curve for the Desicion Tree Model Tuned with Randomized Search CV

y_pred_tuned2 = randomSearch_model.predict_proba(X_test)  # Predict probabilities for all classes
n_classes = len(randomSearch_model.classes_)

# Arrays to store precision, recall, and PR-AUC for each class
precisions = []
recalls = []
pr_aucs = []

# Calculate and plot PR curves for each class
plt.figure(figsize=(8, 6))

for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_test == i, y_pred_tuned2[:, i])
    pr_auc = average_precision_score(y_test == i, y_pred_tuned2[:, i])

    precisions.append(precision)
    recalls.append(recall)
    pr_aucs.append(pr_auc)

    plt.plot(recall, precision, lw=2, label='Class %d (AUC = %0.2f)' % (i, pr_auc))

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.title('Precision-Recall Curve for Desicion Tree')
plt.legend(loc='lower left')
plt.grid(True)
plt.show()

"""<a name="C"></a>
# ***7. Conclusion***
"""

